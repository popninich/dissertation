{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd2809-021d-4cdf-a047-54c5c85d160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ee4d7-578b-49be-a97e-ae881410a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec94a54-7863-44a2-b80e-5473aeec3846",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"amex_credit_card\"\n",
    "\n",
    "# Available scenarios: original, standardization, imbalance_handle\n",
    "SCENARIO = \"imbalance_handle\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53444206-a001-4d61-8485-e61897be0845",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c065df-45ad-42ec-8ac8-c474dd35c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_df = pd.read_csv(\n",
    "#     \"data/amex-default-prediction/train_data.csv\"\n",
    "# )\n",
    "\n",
    "# train_label_df = pd.read_csv(\n",
    "#     \"data/amex-default-prediction/train_labels.csv\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c55ff8-4530-494d-b72a-22ee2f571b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_df.shape, train_label_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6e561-43af-4955-a4eb-9c6571e36919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform left join\n",
    "# df = pd.merge(train_data_df, train_label_df, on='customer_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde077c-7584-41f0-b879-343ac3d53a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [i for i in df.columns if i != \"target\" and i != \"customer_ID\"]\n",
    "\n",
    "# NUMERIC_FEATURES = [i for i in df.select_dtypes(include=['float64', 'int64']).columns if i != \"target\"]\n",
    "\n",
    "# CATEGORICAL_FEATURES = [i for i in FEATURES if i not in NUMERIC_FEATURES]\n",
    "\n",
    "# LABEL = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d737281-5c4f-4bcb-ac18-9a0b5b6e5a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.isna().any(axis=1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36553288-fac2-4c67-b0d5-8b38a9195861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Handle null values\n",
    "# fill_values = {}\n",
    "# for i in NUMERIC_FEATURES:\n",
    "#     fill_values[i] = 0\n",
    "# for i in CATEGORICAL_FEATURES:\n",
    "#     fill_values[i] = \"NULL\"\n",
    "\n",
    "# # Fill null values in the DataFrame using the specified fill values\n",
    "# df = df.fillna(fill_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df1f4c5-7526-42f8-9549-08638986656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.isna().any(axis=1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ffeb8-1e80-4f14-88a1-0ec3fce3c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Due to lack of resources, downsampled to 300k, 10x compared to Taiwan dataset\n",
    "# df = df.sample(n=300000, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f2aa88-63ce-4943-8a43-c6295290759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7251762-a9cf-48bd-ba8c-b32dfb749252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(f\"data/X_y/{DATASET}/df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca679c8c-50f6-47a4-96f9-f9f42bf90233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[FEATURES]\n",
    "# y = df[LABEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b2759f-e239-4de2-8a90-d6a2e1395d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5351f9bb-a468-4a2b-b197-a01f501f4aec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e59a3-c527-4462-b807-950178121db5",
   "metadata": {},
   "source": [
    "### Prepare numeric and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac5190-e320-47ec-a21e-01ca099919b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6361fdf-44dc-4254-b5eb-4fcbdabca3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply label encoder to object columns\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for col in CATEGORICAL_FEATURES:\n",
    "#     X.loc[:, f\"enc_{col}\"] = label_encoder.fit_transform(X.loc[:, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f4615-6c7f-4e30-8cae-00a8017b55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply normalizer\n",
    "# if SCENARIO in (\"standardization\", \"imbalance_handle\"):\n",
    "#     scaler = StandardScaler()\n",
    "    \n",
    "#     # Normalize the numeric columns\n",
    "#     X.loc[:, NUMERIC_FEATURES] = scaler.fit_transform(X.loc[:, NUMERIC_FEATURES])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6bd3e8-9670-42f0-8cd0-0e4d680b7301",
   "metadata": {},
   "source": [
    "### Split train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4247a2-e3cd-48bf-bb96-08644c937fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the data into training, validation, and testing sets: 70, 10, 20%\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    "# )\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_train_val, y_train_val, test_size=0.125, stratify=y_train_val, random_state=SEED\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de139571-f77f-44e5-aa7d-89d2ca326132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if SCENARIO in (\"standardization\", \"imbalance_handle\"):\n",
    "#     X_train.to_csv(f\"data/X_y/{DATASET}/scaled_X_train.csv\")\n",
    "#     y_train.to_csv(f\"data/X_y/{DATASET}/scaled_y_train.csv\")\n",
    "    \n",
    "#     X_val.to_csv(f\"data/X_y/{DATASET}/scaled_X_val.csv\")\n",
    "#     y_val.to_csv(f\"data/X_y/{DATASET}/scaled_y_val.csv\")\n",
    "    \n",
    "#     X_test.to_csv(f\"data/X_y/{DATASET}/scaled_X_test.csv\")\n",
    "#     y_test.to_csv(f\"data/X_y/{DATASET}/scaled_y_test.csv\")\n",
    "    \n",
    "# else:\n",
    "#     X_train.to_csv(f\"data/X_y/{DATASET}/X_train.csv\")\n",
    "#     y_train.to_csv(f\"data/X_y/{DATASET}/y_train.csv\")\n",
    "    \n",
    "#     X_val.to_csv(f\"data/X_y/{DATASET}/X_val.csv\")\n",
    "#     y_val.to_csv(f\"data/X_y/{DATASET}/y_val.csv\")\n",
    "    \n",
    "#     X_test.to_csv(f\"data/X_y/{DATASET}/X_test.csv\")\n",
    "#     y_test.to_csv(f\"data/X_y/{DATASET}/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4227d883-153b-4072-9501-a73b254be102",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCENARIO in (\"standardization\", \"imbalance_handle\"):\n",
    "    X_train = pd.read_csv(f\"data/X_y/{DATASET}/scaled_X_train.csv\", index_col=0)\n",
    "    X_val = pd.read_csv(f\"data/X_y/{DATASET}/scaled_X_val.csv\", index_col=0)\n",
    "    X_test = pd.read_csv(f\"data/X_y/{DATASET}/scaled_X_test.csv\", index_col=0)\n",
    "    \n",
    "    y_train = pd.read_csv(f\"data/X_y/{DATASET}/scaled_y_train.csv\", index_col=0)\n",
    "    y_val = pd.read_csv(f\"data/X_y/{DATASET}/scaled_y_val.csv\", index_col=0)\n",
    "    y_test = pd.read_csv(f\"data/X_y/{DATASET}/scaled_y_test.csv\", index_col=0)\n",
    "\n",
    "else:\n",
    "    X_train = pd.read_csv(f\"data/X_y/{DATASET}/X_train.csv\", index_col=0)\n",
    "    X_val = pd.read_csv(f\"data/X_y/{DATASET}/X_val.csv\", index_col=0)\n",
    "    X_test = pd.read_csv(f\"data/X_y/{DATASET}/X_test.csv\", index_col=0)\n",
    "    \n",
    "    y_train = pd.read_csv(f\"data/X_y/{DATASET}/y_train.csv\", index_col=0)\n",
    "    y_val = pd.read_csv(f\"data/X_y/{DATASET}/y_val.csv\", index_col=0)\n",
    "    y_test = pd.read_csv(f\"data/X_y/{DATASET}/y_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887a965-77c0-43bc-85ea-8deea06c70dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "print()\n",
    "\n",
    "# There are imbalanced data in the y_train\n",
    "print(\"Label Distribution in y_train:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"Label Distribution in y_val:\")\n",
    "print(y_val.value_counts())\n",
    "\n",
    "print(\"Label Distribution in y_test:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278020e-b150-42fe-a5e7-6b86246fa2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [i for i in X_train.columns if i != \"target\" and i != \"customer_ID\"]\n",
    "\n",
    "NUMERIC_FEATURES = [i for i in X_train.select_dtypes(include=['float64', 'int64']).columns if i != \"target\"]\n",
    "\n",
    "CATEGORICAL_FEATURES = [i for i in FEATURES if i not in NUMERIC_FEATURES]\n",
    "\n",
    "LABEL = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c44de1-409c-47b1-a488-72ad0e3c3e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle null values\n",
    "fill_values = {}\n",
    "for i in NUMERIC_FEATURES:\n",
    "    fill_values[i] = 0\n",
    "for i in CATEGORICAL_FEATURES:\n",
    "    fill_values[i] = \"NULL\"\n",
    "\n",
    "# Fill null values in the DataFrame using the specified fill values\n",
    "X_train = X_train.fillna(fill_values)\n",
    "X_val = X_val.fillna(fill_values)\n",
    "X_test = X_test.fillna(fill_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786f58e-6484-47e2-aab9-7d01e3c96648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values\n",
    "X_train[X_train.isna().any(axis=1)].shape, X_val[X_val.isna().any(axis=1)].shape, X_test[X_test.isna().any(axis=1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fcebe2-a733-4d0f-aeab-338664218dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(FEATURES), len(NUMERIC_FEATURES), len(CATEGORICAL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797292dd-d759-405f-be86-0a0f63cd232e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "887cc888-59c3-4953-9945-fc0c7cbe2d61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae2a07f-36b1-4901-9b76-fe42ea91effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae9489-97ef-4fad-b529-7be384a9b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the imbalance handling\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Assuming train_labels contains the integer class labels of your training data.\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train[LABEL].values)\n",
    "\n",
    "# Convert class_weights to a dictionary\n",
    "class_weight_dict = {class_index: weight for class_index, weight in enumerate(class_weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30bf0a8-3f34-4342-8959-5d5a75b020f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"class_weight_dict: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f8277-a04d-4720-b8d1-57092f5b743b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9870abda-809b-4469-bbfe-0162478436c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456a6eb-68be-4fea-a845-76526b1acd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENC_FEATURES = [f\"enc_{i}\" if i in CATEGORICAL_FEATURES else i for i in FEATURES]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f960661-a8d7-450b-8c54-83161c688bb4",
   "metadata": {},
   "source": [
    "Warning: this training process will take about 50-60 mins, due to the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3ddd2-5c87-4e40-b404-ac35e4875e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     logistic_regression = LogisticRegression(class_weight=class_weight_dict)\n",
    "#     decision_tree = DecisionTreeClassifier(class_weight=class_weight_dict)\n",
    "#     random_forest = RandomForestClassifier(class_weight=class_weight_dict)\n",
    "#     svm_classifier = SVC(class_weight=class_weight_dict)\n",
    "# else:  \n",
    "#     logistic_regression = LogisticRegression()\n",
    "#     decision_tree = DecisionTreeClassifier()\n",
    "#     random_forest = RandomForestClassifier()\n",
    "#     svm_classifier = SVC()\n",
    "\n",
    "# # Train the classifiers\n",
    "# logistic_regression.fit(X_train[ENC_FEATURES], y_train)\n",
    "# decision_tree.fit(X_train[ENC_FEATURES], y_train)\n",
    "# random_forest.fit(X_train[ENC_FEATURES], y_train)\n",
    "# svm_classifier.fit(X_train[ENC_FEATURES], y_train)\n",
    "\n",
    "# print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d390e-d96c-47aa-87e6-32cac210a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    logistic_regression = joblib.load(f\"model/{DATASET}/classweight_logistic_regression.joblib\")\n",
    "    decision_tree = joblib.load(f\"model/{DATASET}/classweight_decision_tree.joblib\")\n",
    "    random_forest = joblib.load(f\"model/{DATASET}/classweight_random_forest.joblib\")\n",
    "    svm_classifier = joblib.load(f\"model/{DATASET}/classweight_svm_classifier.joblib\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    logistic_regression = joblib.load(f\"model/{DATASET}/scaled_logistic_regression.joblib\")\n",
    "    decision_tree = joblib.load(f\"model/{DATASET}/scaled_decision_tree.joblib\")\n",
    "    random_forest = joblib.load(f\"model/{DATASET}/scaled_random_forest.joblib\")\n",
    "    svm_classifier = joblib.load(f\"model/{DATASET}/scaled_svm_classifier.joblib\")\n",
    "else:  \n",
    "    logistic_regression = joblib.load(f\"model/{DATASET}/logistic_regression.joblib\")\n",
    "    decision_tree = joblib.load(f\"model/{DATASET}/decision_tree.joblib\")\n",
    "    random_forest = joblib.load(f\"model/{DATASET}/random_forest.joblib\")\n",
    "    svm_classifier = joblib.load(f\"model/{DATASET}/svm_classifier.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec26fc-9533-496f-b74e-38d1c5b998fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "models = {\n",
    "    \"Logistic Regression\": logistic_regression,\n",
    "    \"Decision Tree\": decision_tree,\n",
    "    \"Random Forest\": random_forest,\n",
    "    \"SVM\": svm_classifier\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    y_pred = model.predict(X_test[ENC_FEATURES])\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"Model:\", model_name)\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1-score: {f1:.3f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "    print(confusion_mat)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc479bea-bac4-45ec-914e-0a613979cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     joblib.dump(logistic_regression, f\"model/{DATASET}/classweight_logistic_regression.joblib\")\n",
    "#     joblib.dump(decision_tree, f\"model/{DATASET}/classweight_decision_tree.joblib\")\n",
    "#     joblib.dump(random_forest, f\"model/{DATASET}/classweight_random_forest.joblib\")\n",
    "#     joblib.dump(svm_classifier, f\"model/{DATASET}/classweight_svm_classifier.joblib\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     joblib.dump(logistic_regression, f\"model/{DATASET}/scaled_logistic_regression.joblib\")\n",
    "#     joblib.dump(decision_tree, f\"model/{DATASET}/scaled_decision_tree.joblib\")\n",
    "#     joblib.dump(random_forest, f\"model/{DATASET}/scaled_random_forest.joblib\")\n",
    "#     joblib.dump(svm_classifier, f\"model/{DATASET}/scaled_svm_classifier.joblib\")\n",
    "# else:  \n",
    "#     joblib.dump(logistic_regression, f\"model/{DATASET}/logistic_regression.joblib\")\n",
    "#     joblib.dump(decision_tree, f\"model/{DATASET}/decision_tree.joblib\")\n",
    "#     joblib.dump(random_forest, f\"model/{DATASET}/random_forest.joblib\")\n",
    "#     joblib.dump(svm_classifier, f\"model/{DATASET}/svm_classifier.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d8e9c-9291-430f-8050-14228edb505c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d65b5a2c-3995-403c-87af-eaf569f2c334",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96449cf8-469c-4c03-9c73-a2db07b43b79",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed1234-3120-4a0b-b25b-e4a9bc005dc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train Neural networks\n",
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.Dense(32, activation='relu', input_shape=(len(FEATURES),)))\n",
    "# model.add(keras.layers.Dense(8, activation='relu'))\n",
    "# model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# early_stopper = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     history = model.fit(\n",
    "#         X_train[ENC_FEATURES],\n",
    "#         y_train, \n",
    "#         epochs=100, \n",
    "#         batch_size=1024,\n",
    "#         validation_data=(X_val[ENC_FEATURES], y_val), \n",
    "#         callbacks=[early_stopper], \n",
    "#         class_weight=class_weight_dict\n",
    "#     )\n",
    "# else:\n",
    "#     history = model.fit(\n",
    "#         X_train[ENC_FEATURES],\n",
    "#         y_train, \n",
    "#         epochs=100, \n",
    "#         batch_size=1024,\n",
    "#         validation_data=(X_val[ENC_FEATURES], y_val), \n",
    "#         callbacks=[early_stopper]\n",
    "#     )\n",
    "\n",
    "# # Plot the loss history\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f28b7-4635-4615-848f-8fea18f7dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    model = tf.keras.models.load_model(f\"model/{DATASET}/classweight_neural_network.h5\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    model = tf.keras.models.load_model(f\"model/{DATASET}/scaled_neural_network.h5\")\n",
    "else:  \n",
    "    model = tf.keras.models.load_model(f\"model/{DATASET}/neural_network.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494be9b5-b817-47a5-802d-938b0f34a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test[ENC_FEATURES])\n",
    "\n",
    "# Define the threshold for binary classification\n",
    "threshold = 0.5\n",
    "y_pred = (predictions >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a755e6-1269-4669-a0da-a1378e7028dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     model.save(f\"model/{DATASET}/classweight_neural_network.h5\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     model.save(f\"model/{DATASET}/scaled_neural_network.h5\")\n",
    "# else:  \n",
    "#     model.save(f\"model/{DATASET}/neural_network.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71566f-4f8d-435a-9bd6-52bacdfca706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "749bc3c9-bd68-4071-8cd1-bb54c3674d4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774415de-9713-43c3-a6ca-872209013270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train CNN\n",
    "# cnn_model = Sequential()\n",
    "# cnn_model.add(Conv1D(32, 3, activation='relu', input_shape=(len(FEATURES), 1)))\n",
    "# cnn_model.add(MaxPooling1D(2))\n",
    "# cnn_model.add(Flatten())\n",
    "# cnn_model.add(Dense(128, activation='relu'))\n",
    "# cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "# cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Convert the data to the required input shape for CNN (num_samples, num_features, num_channels)\n",
    "# X_train_cnn = np.expand_dims(X_train[ENC_FEATURES], axis=2)\n",
    "# X_val_cnn = np.expand_dims(X_val[ENC_FEATURES], axis=2)\n",
    "\n",
    "# early_stopper = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     history = cnn_model.fit(\n",
    "#         X_train_cnn, y_train, \n",
    "#         epochs=100, \n",
    "#         batch_size=1024,\n",
    "#         validation_data=(X_val_cnn, y_val), \n",
    "#         callbacks=[early_stopper],\n",
    "#         class_weight=class_weight_dict\n",
    "#     )\n",
    "# else:\n",
    "#     history = cnn_model.fit(\n",
    "#         X_train_cnn, y_train, \n",
    "#         epochs=100, \n",
    "#         batch_size=1024,\n",
    "#         validation_data=(X_val_cnn, y_val), \n",
    "#         callbacks=[early_stopper]\n",
    "#     )\n",
    "\n",
    "# # Plot the loss history\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a6cbe-0885-4e54-b89a-433581b4c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    cnn_model = tf.keras.models.load_model(f\"model/{DATASET}/classweight_cnn.h5\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    cnn_model = tf.keras.models.load_model(f\"model/{DATASET}/scaled_cnn.h5\")\n",
    "else:  \n",
    "    cnn_model = tf.keras.models.load_model(f\"model/{DATASET}/cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e87664-fa6b-410e-9575-5dc4b9f5c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "X_test_cnn = np.expand_dims(X_test[ENC_FEATURES], axis=2)\n",
    "predictions = cnn_model.predict(X_test_cnn)\n",
    "\n",
    "# Define the threshold for binary classification\n",
    "threshold = 0.5\n",
    "y_pred = (predictions >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "print(confusion_mat)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e485c1d-99f1-44b0-aef3-26dbdf869f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     cnn_model.save(f\"model/{DATASET}/classweight_cnn.h5\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     cnn_model.save(f\"model/{DATASET}/scaled_cnn.h5\")\n",
    "# else:  \n",
    "#     cnn_model.save(f\"model/{DATASET}/cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1396c20-3536-49e6-8d5e-9417e6302587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "199880c1-b88a-4ed5-afaa-3e4fb836b3c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b71aedf-b528-40f3-b10d-c225108f2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input data for the LSTM model\n",
    "input_dim = len(FEATURES)\n",
    "input_shape = (input_dim, 1)  # Add an extra dimension for LSTM input\n",
    "\n",
    "# Reshape the input data for LSTM\n",
    "X_train_lstm = X_train[ENC_FEATURES].values.reshape((-1, input_dim, 1))\n",
    "X_val_lstm = X_val[ENC_FEATURES].values.reshape((-1, input_dim, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc118b4b-01f6-4b5d-9592-20965c20ef01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train the LSTMs\n",
    "# inputs = Input(shape=input_shape)\n",
    "# lstm_layer = LSTM(32, activation='relu')(inputs)\n",
    "# dropout_layer = Dropout(0.1)(lstm_layer)\n",
    "# outputs = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "# lstm_model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# # Compile the model\n",
    "# lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# early_stopper = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     history = lstm_model.fit(\n",
    "#         X_train_lstm, y_train, \n",
    "#         epochs=20, \n",
    "#         validation_data=(X_val_lstm, y_val), \n",
    "#         callbacks=[early_stopper],\n",
    "#         class_weight=class_weight_dict\n",
    "#     )\n",
    "# else:\n",
    "#     history = lstm_model.fit(\n",
    "#         X_train_lstm, y_train, \n",
    "#         epochs=20, \n",
    "#         validation_data=(X_val_lstm, y_val), \n",
    "#         callbacks=[early_stopper]\n",
    "#     )\n",
    "\n",
    "# # Plot the loss history\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf2126-db4e-4820-9d93-41e170d6ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    lstm_model = tf.keras.models.load_model(f\"model/{DATASET}/classweight_lstm.h5\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    lstm_model = tf.keras.models.load_model(f\"model/{DATASET}/scaled_lstm.h5\")\n",
    "else:  \n",
    "    lstm_model = tf.keras.models.load_model(f\"model/{DATASET}/lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb0e0b-0ba4-475f-ab6c-dc01d56daa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "X_test_lstm = X_test[ENC_FEATURES].values.reshape((-1, input_dim, 1))\n",
    "predictions = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "# Define the threshold for binary classification\n",
    "threshold = 0.5\n",
    "y_pred = (predictions >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "print(confusion_mat)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d20bf47-c0a7-45c2-9920-4147a8aa205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     lstm_model.save(f\"model/{DATASET}/classweight_lstm.h5\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     lstm_model.save(f\"model/{DATASET}/scaled_lstm.h5\")\n",
    "# else:  \n",
    "#     lstm_model.save(f\"model/{DATASET}/lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3152cef5-73a5-466f-b296-7029f63e1ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "917e7400-ef9a-44d8-8e27-067f4a66e644",
   "metadata": {},
   "source": [
    "### TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878944f-60a5-45b6-b31a-9c33bd1b91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "from tabtransformertf.models.tabtransformer import TabTransformer\n",
    "from tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73daca3-ea03-4b50-b2c9-fba19c5e7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(\n",
    "    dataframe: pd.DataFrame,\n",
    "    target: str = None,\n",
    "    shuffle: bool = True,\n",
    "    batch_size: int = 512,\n",
    "):\n",
    "    df = dataframe.copy()\n",
    "    if target:\n",
    "        labels = df.pop(target)\n",
    "        dataset = {}\n",
    "        for key, value in df.items():\n",
    "            dataset[key] = tf.expand_dims(value, axis=1) # Expand dimension similar to value[:, tf.newaxis]\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((dict(dataset), labels))\n",
    "    else:\n",
    "        dataset = {}\n",
    "        for key, value in df.items():\n",
    "            dataset[key] = tf.expand_dims(value, axis=1)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(dict(dataset))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8168b5-5413-4e30-9d99-84293fb69f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train[FEATURES], y_train], axis=1)\n",
    "val_df = pd.concat([X_val[FEATURES], y_val], axis=1)\n",
    "test_df = pd.concat([X_test[FEATURES], y_test], axis=1)\n",
    "\n",
    "# Set data types\n",
    "train_df[CATEGORICAL_FEATURES] = train_df[CATEGORICAL_FEATURES].astype(str)\n",
    "val_df[CATEGORICAL_FEATURES] = val_df[CATEGORICAL_FEATURES].astype(str)\n",
    "test_df[CATEGORICAL_FEATURES] = test_df[CATEGORICAL_FEATURES].astype(str)\n",
    "\n",
    "train_df[NUMERIC_FEATURES] = train_df[NUMERIC_FEATURES].astype(float)\n",
    "val_df[NUMERIC_FEATURES] = val_df[NUMERIC_FEATURES].astype(float)\n",
    "test_df[NUMERIC_FEATURES] = test_df[NUMERIC_FEATURES].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56e6e8-3713-4a98-be87-bd50872c4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To TF Dataset\n",
    "train_dataset = df_to_dataset(train_df, LABEL, batch_size=1024, shuffle=False)\n",
    "val_dataset = df_to_dataset(val_df, LABEL, batch_size=1024, shuffle=False) # No shuffle\n",
    "test_dataset = df_to_dataset(test_df, LABEL, batch_size=1024, shuffle=False) # No shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b91bde-376c-41a2-9436-9ce0b8b3aa5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "053afe95-a463-4bfd-935f-645c07d01b72",
   "metadata": {},
   "source": [
    "#### fttransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa150422-ea17-4228-ba58-0b8b3cc1db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the tab transformer\n",
    "# ft_linear_encoder = FTTransformerEncoder(\n",
    "#     numerical_features=NUMERIC_FEATURES,\n",
    "#     categorical_features=CATEGORICAL_FEATURES,\n",
    "#     numerical_data=train_df[NUMERIC_FEATURES].values,\n",
    "#     categorical_data=train_df[CATEGORICAL_FEATURES].values,\n",
    "#     y=None,\n",
    "#     numerical_embedding_type='linear',\n",
    "#     embedding_dim=16,\n",
    "#     depth=4,\n",
    "#     heads=8,\n",
    "#     attn_dropout=0.2,\n",
    "#     ff_dropout=0.2,\n",
    "#     explainable=True\n",
    "# )\n",
    "\n",
    "# # Pass the encoder to the model\n",
    "# ft_linear_transformer = FTTransformer(\n",
    "#     encoder=ft_linear_encoder,\n",
    "#     out_dim=1,\n",
    "#     out_activation='sigmoid',\n",
    "# )\n",
    "\n",
    "# ft_linear_transformer.compile(\n",
    "#     optimizer=\"adam\",\n",
    "#     loss=\"binary_crossentropy\",\n",
    "#     metrics=[\"accuracy\"]\n",
    "# )\n",
    "\n",
    "# early = EarlyStopping(monitor=\"val_output_loss\", mode=\"min\", patience=10, restore_best_weights=True)\n",
    "# callback_list = [early]\n",
    "\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     ft_linear_history = ft_linear_transformer.fit(\n",
    "#         train_dataset, \n",
    "#         epochs=20, \n",
    "#         validation_data=val_dataset,\n",
    "#         callbacks=callback_list,\n",
    "#         class_weight=class_weight_dict\n",
    "#     )\n",
    "# else:\n",
    "#     ft_linear_history = ft_linear_transformer.fit(\n",
    "#         train_dataset, \n",
    "#         epochs=20, \n",
    "#         validation_data=val_dataset,\n",
    "#         callbacks=callback_list\n",
    "#     )\n",
    "\n",
    "\n",
    "# # Plot the loss history\n",
    "# plt.plot(ft_linear_history.history['output_accuracy'])\n",
    "# plt.plot(ft_linear_history.history['val_output_accuracy'])\n",
    "# plt.title('Model Acc')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Acc')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5ebbc-efbd-4110-9618-af675e0e0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    ft_linear_transformer = tf.keras.models.load_model(f\"model/{DATASET}/classweight_ft_linear_transformer\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    ft_linear_transformer = tf.keras.models.load_model(f\"model/{DATASET}/scaled_ft_linear_transformer\")\n",
    "else:  \n",
    "    ft_linear_transformer = tf.keras.models.load_model(f\"model/{DATASET}/ft_linear_transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a01474-25dc-488f-b369-2fb54649f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "predictions = ft_linear_transformer.predict(test_dataset)\n",
    "y = y_test\n",
    "\n",
    "# Define the threshold for binary classification\n",
    "threshold = 0.5\n",
    "y_pred = (predictions[\"output\"] >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "roc_auc = roc_auc_score(y, y_pred)\n",
    "confusion_mat = confusion_matrix(y, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "print(confusion_mat)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5b80c9-4c11-41b9-abf4-8830a7f10cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     ft_linear_transformer.save(f\"model/{DATASET}/classweight_ft_linear_transformer\", save_format=\"tf\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     ft_linear_transformer.save(f\"model/{DATASET}/scaled_ft_linear_transformer\", save_format=\"tf\")\n",
    "# else:  \n",
    "#     ft_linear_transformer.save(f\"model/{DATASET}/ft_linear_transformer\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67b2cb-d1bd-4305-92bc-3c659af33437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
