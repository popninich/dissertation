{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd2809-021d-4cdf-a047-54c5c85d160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ee4d7-578b-49be-a97e-ae881410a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37e639-2eb0-4b5c-8191-67c45bd3d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"german_credit_card\"\n",
    "\n",
    "# Available scenarios: original, standardization, imbalance_handle\n",
    "SCENARIO = \"imbalance_handle\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53444206-a001-4d61-8485-e61897be0845",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c065df-45ad-42ec-8ac8-c474dd35c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\n",
    "#     \"data/german_credit.csv\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c55ff8-4530-494d-b72a-22ee2f571b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372540fd-97b5-4f3f-948a-b051281446d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"credit_risk\"] = df[\"credit_risk\"].replace({'good': 1, 'bad': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde077c-7584-41f0-b879-343ac3d53a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    'status', 'duration', 'credit_history', 'purpose', 'amount', 'savings',\n",
    "    'employment_duration', 'installment_rate', 'personal_status_sex',\n",
    "    'other_debtors', 'present_residence', 'property', 'age',\n",
    "    'other_installment_plans', 'housing', 'number_credits', 'job',\n",
    "    'people_liable', 'telephone', 'foreign_worker'\n",
    "]\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    'duration',\n",
    "    'amount',\n",
    "    'age',\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [i for i in FEATURES if i not in NUMERIC_FEATURES]\n",
    "\n",
    "LABEL = \"credit_risk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca679c8c-50f6-47a4-96f9-f9f42bf90233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[FEATURES]\n",
    "# y = df[LABEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9cd7b-47d1-4aeb-a5bb-c6de0932411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the shape of the training and testing sets\n",
    "# print(\"Dataset shape:\", X.shape, y.shape)\n",
    "\n",
    "# # There are imbalanced data in the y_train\n",
    "# print(\"Label Distribution in y_train:\")\n",
    "# print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6defa523-2887-48b6-8a54-3a37f32b94dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5351f9bb-a468-4a2b-b197-a01f501f4aec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac5190-e320-47ec-a21e-01ca099919b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6361fdf-44dc-4254-b5eb-4fcbdabca3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply label encoder to object columns\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for col in CATEGORICAL_FEATURES:\n",
    "#     X.loc[:, f\"enc_{col}\"] = label_encoder.fit_transform(X.loc[:, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f4615-6c7f-4e30-8cae-00a8017b55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply normalizer\n",
    "# if SCENARIO in (\"standardization\", \"imbalance_handle\"):\n",
    "#     scaler = StandardScaler()\n",
    "    \n",
    "#     # Normalize the numeric columns\n",
    "#     X.loc[:, NUMERIC_FEATURES] = scaler.fit_transform(X.loc[:, NUMERIC_FEATURES])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6bd3e8-9670-42f0-8cd0-0e4d680b7301",
   "metadata": {},
   "source": [
    "### Split train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4247a2-e3cd-48bf-bb96-08644c937fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the data into training, validation, and testing sets: 70, 10, 20%\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    "# )\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_train_val, y_train_val, test_size=0.125, stratify=y_train_val, random_state=SEED\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de139571-f77f-44e5-aa7d-89d2ca326132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if SCENARIO in (\"standardization\", \"imbalance_handle\"):\n",
    "#     X_train.to_csv(f\"data/X_y/{DATASET}/scaled_X_train.csv\")\n",
    "#     y_train.to_csv(f\"data/X_y/{DATASET}/scaled_y_train.csv\")\n",
    "    \n",
    "#     X_val.to_csv(f\"data/X_y/{DATASET}/scaled_X_val.csv\")\n",
    "#     y_val.to_csv(f\"data/X_y/{DATASET}/scaled_y_val.csv\")\n",
    "    \n",
    "#     X_test.to_csv(f\"data/X_y/{DATASET}/scaled_X_test.csv\")\n",
    "#     y_test.to_csv(f\"data/X_y/{DATASET}/scaled_y_test.csv\")\n",
    "    \n",
    "# else:\n",
    "#     X_train.to_csv(f\"data/X_y/{DATASET}/X_train.csv\")\n",
    "#     y_train.to_csv(f\"data/X_y/{DATASET}/y_train.csv\")\n",
    "    \n",
    "#     X_val.to_csv(f\"data/X_y/{DATASET}/X_val.csv\")\n",
    "#     y_val.to_csv(f\"data/X_y/{DATASET}/y_val.csv\")\n",
    "    \n",
    "#     X_test.to_csv(f\"data/X_y/{DATASET}/X_test.csv\")\n",
    "#     y_test.to_csv(f\"data/X_y/{DATASET}/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4227d883-153b-4072-9501-a73b254be102",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCENARIO in (\"standardization\", \"imbalance_handle\"):\n",
    "    X_train = pd.read_csv(f\"data/X_y/{DATASET}/scaled_X_train.csv\", index_col=0)\n",
    "    X_val = pd.read_csv(f\"data/X_y/{DATASET}/scaled_X_val.csv\", index_col=0)\n",
    "    X_test = pd.read_csv(f\"data/X_y/{DATASET}/scaled_X_test.csv\", index_col=0)\n",
    "    \n",
    "    y_train = pd.read_csv(f\"data/X_y/{DATASET}/scaled_y_train.csv\", index_col=0)\n",
    "    y_val = pd.read_csv(f\"data/X_y/{DATASET}/scaled_y_val.csv\", index_col=0)\n",
    "    y_test = pd.read_csv(f\"data/X_y/{DATASET}/scaled_y_test.csv\", index_col=0)\n",
    "\n",
    "else:\n",
    "    X_train = pd.read_csv(f\"data/X_y/{DATASET}/X_train.csv\", index_col=0)\n",
    "    X_val = pd.read_csv(f\"data/X_y/{DATASET}/X_val.csv\", index_col=0)\n",
    "    X_test = pd.read_csv(f\"data/X_y/{DATASET}/X_test.csv\", index_col=0)\n",
    "    \n",
    "    y_train = pd.read_csv(f\"data/X_y/{DATASET}/y_train.csv\", index_col=0)\n",
    "    y_val = pd.read_csv(f\"data/X_y/{DATASET}/y_val.csv\", index_col=0)\n",
    "    y_test = pd.read_csv(f\"data/X_y/{DATASET}/y_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260beaa-4043-482d-9b62-0aa38df132e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "print()\n",
    "\n",
    "# There are imbalanced data in the y_train\n",
    "print(\"Label Distribution in y_train:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86d01b-0082-440d-8342-130bb99097d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "887cc888-59c3-4953-9945-fc0c7cbe2d61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae2a07f-36b1-4901-9b76-fe42ea91effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae9489-97ef-4fad-b529-7be384a9b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the imbalance handling\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Assuming train_labels contains the integer class labels of your training data.\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train[LABEL].values)\n",
    "\n",
    "# Convert class_weights to a dictionary\n",
    "class_weight_dict = {class_index: weight for class_index, weight in enumerate(class_weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30bf0a8-3f34-4342-8959-5d5a75b020f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"class_weight_dict: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9870abda-809b-4469-bbfe-0162478436c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456a6eb-68be-4fea-a845-76526b1acd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENC_FEATURES = [f\"enc_{i}\" if i in CATEGORICAL_FEATURES else i for i in FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72bff83-f329-4ef6-a3e5-c469a6c846ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     logistic_regression = LogisticRegression(class_weight=class_weight_dict)\n",
    "#     decision_tree = DecisionTreeClassifier(class_weight=class_weight_dict)\n",
    "#     random_forest = RandomForestClassifier(class_weight=class_weight_dict)\n",
    "#     svm_classifier = SVC(class_weight=class_weight_dict)\n",
    "# else:  \n",
    "#     logistic_regression = LogisticRegression()\n",
    "#     decision_tree = DecisionTreeClassifier()\n",
    "#     random_forest = RandomForestClassifier()\n",
    "#     svm_classifier = SVC()\n",
    "\n",
    "# # Train the classifiers\n",
    "# logistic_regression.fit(X_train[ENC_FEATURES], y_train)\n",
    "# decision_tree.fit(X_train[ENC_FEATURES], y_train)\n",
    "# random_forest.fit(X_train[ENC_FEATURES], y_train)\n",
    "# svm_classifier.fit(X_train[ENC_FEATURES], y_train)\n",
    "\n",
    "# print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869c772-f006-4e89-a0f5-b78db6ba2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    logistic_regression = joblib.load(f\"model/{DATASET}/classweight_logistic_regression.joblib\")\n",
    "    decision_tree = joblib.load(f\"model/{DATASET}/classweight_decision_tree.joblib\")\n",
    "    random_forest = joblib.load(f\"model/{DATASET}/classweight_random_forest.joblib\")\n",
    "    svm_classifier = joblib.load(f\"model/{DATASET}/classweight_svm_classifier.joblib\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    logistic_regression = joblib.load(f\"model/{DATASET}/scaled_logistic_regression.joblib\")\n",
    "    decision_tree = joblib.load(f\"model/{DATASET}/scaled_decision_tree.joblib\")\n",
    "    random_forest = joblib.load(f\"model/{DATASET}/scaled_random_forest.joblib\")\n",
    "    svm_classifier = joblib.load(f\"model/{DATASET}/scaled_svm_classifier.joblib\")\n",
    "else:  \n",
    "    logistic_regression = joblib.load(f\"model/{DATASET}/logistic_regression.joblib\")\n",
    "    decision_tree = joblib.load(f\"model/{DATASET}/decision_tree.joblib\")\n",
    "    random_forest = joblib.load(f\"model/{DATASET}/random_forest.joblib\")\n",
    "    svm_classifier = joblib.load(f\"model/{DATASET}/svm_classifier.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec26fc-9533-496f-b74e-38d1c5b998fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "models = {\n",
    "    \"Logistic Regression\": logistic_regression,\n",
    "    \"Decision Tree\": decision_tree,\n",
    "    \"Random Forest\": random_forest,\n",
    "    \"SVM\": svm_classifier\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    y_pred = model.predict(X_test[ENC_FEATURES])\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"Model:\", model_name)\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1-score: {f1:.3f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2a575-a396-49ba-8944-1af2d6336943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     joblib.dump(logistic_regression, f\"model/{DATASET}/classweight_logistic_regression.joblib\")\n",
    "#     joblib.dump(decision_tree, f\"model/{DATASET}/classweight_decision_tree.joblib\")\n",
    "#     joblib.dump(random_forest, f\"model/{DATASET}/classweight_random_forest.joblib\")\n",
    "#     joblib.dump(svm_classifier, f\"model/{DATASET}/classweight_svm_classifier.joblib\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     joblib.dump(logistic_regression, f\"model/{DATASET}/scaled_logistic_regression.joblib\")\n",
    "#     joblib.dump(decision_tree, f\"model/{DATASET}/scaled_decision_tree.joblib\")\n",
    "#     joblib.dump(random_forest, f\"model/{DATASET}/scaled_random_forest.joblib\")\n",
    "#     joblib.dump(svm_classifier, f\"model/{DATASET}/scaled_svm_classifier.joblib\")\n",
    "# else:  \n",
    "#     joblib.dump(logistic_regression, f\"model/{DATASET}/logistic_regression.joblib\")\n",
    "#     joblib.dump(decision_tree, f\"model/{DATASET}/decision_tree.joblib\")\n",
    "#     joblib.dump(random_forest, f\"model/{DATASET}/random_forest.joblib\")\n",
    "#     joblib.dump(svm_classifier, f\"model/{DATASET}/svm_classifier.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3ac97-d9d0-4e8b-84e4-c5607c28fda5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d65b5a2c-3995-403c-87af-eaf569f2c334",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96449cf8-469c-4c03-9c73-a2db07b43b79",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed1234-3120-4a0b-b25b-e4a9bc005dc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train Neural networks\n",
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.Dense(32, activation='relu', input_shape=(len(FEATURES),)))\n",
    "# model.add(keras.layers.Dense(8, activation='relu'))\n",
    "# model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# early_stopper = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=50, restore_best_weights=True)\n",
    "\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     history = model.fit(\n",
    "#         X_train[ENC_FEATURES],\n",
    "#         y_train, \n",
    "#         epochs=200, \n",
    "#         validation_data=(X_val[ENC_FEATURES], y_val), \n",
    "#         callbacks=[early_stopper], \n",
    "#         class_weight=class_weight_dict\n",
    "#     )\n",
    "# else:\n",
    "#     history = model.fit(\n",
    "#         X_train[ENC_FEATURES],\n",
    "#         y_train, \n",
    "#         epochs=200, \n",
    "#         validation_data=(X_val[ENC_FEATURES], y_val), \n",
    "#         callbacks=[early_stopper]\n",
    "#     )\n",
    "\n",
    "# # Plot the loss history\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773cd5a7-5786-405f-83ff-47b733b05cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    model = tf.keras.models.load_model(f\"model/{DATASET}/classweight_neural_network.h5\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    model = tf.keras.models.load_model(f\"model/{DATASET}/scaled_neural_network.h5\")\n",
    "else:  \n",
    "    model = tf.keras.models.load_model(f\"model/{DATASET}/neural_network.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494be9b5-b817-47a5-802d-938b0f34a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test[ENC_FEATURES])\n",
    "\n",
    "# Define the threshold for binary classification\n",
    "threshold = 0.5\n",
    "y_pred = (predictions >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a755e6-1269-4669-a0da-a1378e7028dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     model.save(f\"model/{DATASET}/classweight_neural_network.h5\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     model.save(f\"model/{DATASET}/scaled_neural_network.h5\")\n",
    "# else:  \n",
    "#     model.save(f\"model/{DATASET}/neural_network.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749bc3c9-bd68-4071-8cd1-bb54c3674d4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774415de-9713-43c3-a6ca-872209013270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train CNN\n",
    "# cnn_model = Sequential()\n",
    "# cnn_model.add(Conv1D(32, 3, activation='relu', input_shape=(len(FEATURES), 1)))\n",
    "# cnn_model.add(MaxPooling1D(2))\n",
    "# cnn_model.add(Flatten())\n",
    "# cnn_model.add(Dense(128, activation='relu'))\n",
    "# cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "# cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Convert the data to the required input shape for CNN (num_samples, num_features, num_channels)\n",
    "# X_train_cnn = np.expand_dims(X_train[ENC_FEATURES], axis=2)\n",
    "# X_val_cnn = np.expand_dims(X_val[ENC_FEATURES], axis=2)\n",
    "\n",
    "# early_stopper = EarlyStopping(patience=50, restore_best_weights=True)\n",
    "\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     history = cnn_model.fit(\n",
    "#         X_train_cnn, y_train, \n",
    "#         epochs=500, \n",
    "#         validation_data=(X_val_cnn, y_val), \n",
    "#         callbacks=[early_stopper],\n",
    "#         class_weight=class_weight_dict\n",
    "#     )\n",
    "# else:\n",
    "#     history = cnn_model.fit(\n",
    "#         X_train_cnn, y_train, \n",
    "#         epochs=500, \n",
    "#         validation_data=(X_val_cnn, y_val), \n",
    "#         callbacks=[early_stopper]\n",
    "#     )\n",
    "\n",
    "# # Plot the loss history\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74edc54-f8b5-4690-a466-dee75b4cee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    cnn_model = tf.keras.models.load_model(f\"model/{DATASET}/classweight_cnn.h5\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    cnn_model = tf.keras.models.load_model(f\"model/{DATASET}/scaled_cnn.h5\")\n",
    "else:  \n",
    "    cnn_model = tf.keras.models.load_model(f\"model/{DATASET}/cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e87664-fa6b-410e-9575-5dc4b9f5c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "X_test_cnn = np.expand_dims(X_test[ENC_FEATURES], axis=2)\n",
    "predictions = cnn_model.predict(X_test_cnn)\n",
    "\n",
    "# Define the threshold for binary classification\n",
    "threshold = 0.5\n",
    "y_pred = (predictions >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8b6df8-beb1-4e14-9db1-37b2c78c6489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     cnn_model.save(f\"model/{DATASET}/classweight_cnn.h5\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     cnn_model.save(f\"model/{DATASET}/scaled_cnn.h5\")\n",
    "# else:  \n",
    "#     cnn_model.save(f\"model/{DATASET}/cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1396c20-3536-49e6-8d5e-9417e6302587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "199880c1-b88a-4ed5-afaa-3e4fb836b3c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c67b73-4bb2-4936-a1c2-25e491edf79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input data for the LSTM model\n",
    "input_dim = len(FEATURES)\n",
    "input_shape = (input_dim, 1)  # Add an extra dimension for LSTM input\n",
    "\n",
    "# Reshape the input data for LSTM\n",
    "X_train_lstm = X_train[ENC_FEATURES].values.reshape((-1, input_dim, 1))\n",
    "X_val_lstm = X_val[ENC_FEATURES].values.reshape((-1, input_dim, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc118b4b-01f6-4b5d-9592-20965c20ef01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train the LSTMs\n",
    "# inputs = Input(shape=input_shape)\n",
    "# lstm_layer = LSTM(32, activation='relu')(inputs)\n",
    "# dropout_layer = Dropout(0.1)(lstm_layer)\n",
    "# outputs = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "# lstm_model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# # Compile the model\n",
    "# lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# early_stopper = EarlyStopping(patience=15, restore_best_weights=True)\n",
    "\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     history = lstm_model.fit(\n",
    "#         X_train_lstm, y_train, \n",
    "#         epochs=100, \n",
    "#         validation_data=(X_val_lstm, y_val), \n",
    "#         callbacks=[early_stopper],\n",
    "#         class_weight=class_weight_dict\n",
    "#     )\n",
    "# else:\n",
    "#     history = lstm_model.fit(\n",
    "#         X_train_lstm, y_train, \n",
    "#         epochs=100, \n",
    "#         validation_data=(X_val_lstm, y_val), \n",
    "#         callbacks=[early_stopper]\n",
    "#     )\n",
    "\n",
    "# # Plot the loss history\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e92385-b30a-40b1-8612-5e4c9c43b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    lstm_model = tf.keras.models.load_model(f\"model/{DATASET}/classweight_lstm.h5\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    lstm_model = tf.keras.models.load_model(f\"model/{DATASET}/scaled_lstm.h5\")\n",
    "else:  \n",
    "    lstm_model = tf.keras.models.load_model(f\"model/{DATASET}/lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb0e0b-0ba4-475f-ab6c-dc01d56daa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "X_test_lstm = X_test[ENC_FEATURES].values.reshape((-1, input_dim, 1))\n",
    "predictions = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "# Define the threshold for binary classification\n",
    "threshold = 0.5\n",
    "y_pred = (predictions >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3152cef5-73a5-466f-b296-7029f63e1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     lstm_model.save(f\"model/{DATASET}/classweight_lstm.h5\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     lstm_model.save(f\"model/{DATASET}/scaled_lstm.h5\")\n",
    "# else:  \n",
    "#     lstm_model.save(f\"model/{DATASET}/lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed564f64-9f57-4b14-980b-e46396990a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "917e7400-ef9a-44d8-8e27-067f4a66e644",
   "metadata": {},
   "source": [
    "### TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878944f-60a5-45b6-b31a-9c33bd1b91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "from tabtransformertf.models.tabtransformer import TabTransformer\n",
    "from tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73daca3-ea03-4b50-b2c9-fba19c5e7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(\n",
    "    dataframe: pd.DataFrame,\n",
    "    target: str = None,\n",
    "    shuffle: bool = True,\n",
    "    batch_size: int = 512,\n",
    "):\n",
    "    df = dataframe.copy()\n",
    "    if target:\n",
    "        labels = df.pop(target)\n",
    "        dataset = {}\n",
    "        for key, value in df.items():\n",
    "            dataset[key] = tf.expand_dims(value, axis=1) # Expand dimension similar to value[:, tf.newaxis]\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((dict(dataset), labels))\n",
    "    else:\n",
    "        dataset = {}\n",
    "        for key, value in df.items():\n",
    "            dataset[key] = tf.expand_dims(value, axis=1)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(dict(dataset))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8168b5-5413-4e30-9d99-84293fb69f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train[FEATURES], y_train], axis=1)\n",
    "val_df = pd.concat([X_val[FEATURES], y_val], axis=1)\n",
    "test_df = pd.concat([X_test[FEATURES], y_test], axis=1)\n",
    "\n",
    "# Set data types\n",
    "train_df[CATEGORICAL_FEATURES] = train_df[CATEGORICAL_FEATURES].astype(str)\n",
    "val_df[CATEGORICAL_FEATURES] = val_df[CATEGORICAL_FEATURES].astype(str)\n",
    "test_df[CATEGORICAL_FEATURES] = test_df[CATEGORICAL_FEATURES].astype(str)\n",
    "\n",
    "train_df[NUMERIC_FEATURES] = train_df[NUMERIC_FEATURES].astype(float)\n",
    "val_df[NUMERIC_FEATURES] = val_df[NUMERIC_FEATURES].astype(float)\n",
    "test_df[NUMERIC_FEATURES] = test_df[NUMERIC_FEATURES].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56e6e8-3713-4a98-be87-bd50872c4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To TF Dataset\n",
    "train_dataset = df_to_dataset(train_df, LABEL, batch_size=32, shuffle=False)\n",
    "val_dataset = df_to_dataset(val_df, LABEL, batch_size=32, shuffle=False) # No shuffle\n",
    "test_dataset = df_to_dataset(test_df, LABEL, batch_size=32, shuffle=False) # No shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b91bde-376c-41a2-9436-9ce0b8b3aa5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "053afe95-a463-4bfd-935f-645c07d01b72",
   "metadata": {},
   "source": [
    "#### fttransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa150422-ea17-4228-ba58-0b8b3cc1db81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train the tab transformer\n",
    "# ft_linear_encoder = FTTransformerEncoder(\n",
    "#     numerical_features=NUMERIC_FEATURES,\n",
    "#     categorical_features=CATEGORICAL_FEATURES,\n",
    "#     numerical_data=train_df[NUMERIC_FEATURES].values,\n",
    "#     categorical_data=train_df[CATEGORICAL_FEATURES].values,\n",
    "#     y=None,\n",
    "#     numerical_embedding_type='linear',\n",
    "#     embedding_dim=16,\n",
    "#     depth=4,\n",
    "#     heads=8,\n",
    "#     attn_dropout=0.2,\n",
    "#     ff_dropout=0.2,\n",
    "#     explainable=True\n",
    "# )\n",
    "\n",
    "# # Pass the encoder to the model\n",
    "# ft_linear_transformer = FTTransformer(\n",
    "#     encoder=ft_linear_encoder,\n",
    "#     out_dim=1,\n",
    "#     out_activation='sigmoid',\n",
    "# )\n",
    "\n",
    "# ft_linear_transformer.compile(\n",
    "#     optimizer=\"adam\",\n",
    "#     loss=\"binary_crossentropy\",\n",
    "#     metrics=[\"accuracy\"]\n",
    "# )\n",
    "\n",
    "# early = EarlyStopping(monitor=\"val_output_loss\", mode=\"min\", patience=20, restore_best_weights=True)\n",
    "# callback_list = [early]\n",
    "\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     ft_linear_history = ft_linear_transformer.fit(\n",
    "#         train_dataset, \n",
    "#         epochs=200, \n",
    "#         validation_data=val_dataset,\n",
    "#         callbacks=callback_list,\n",
    "#         class_weight=class_weight_dict\n",
    "#     )\n",
    "# else:\n",
    "#     ft_linear_history = ft_linear_transformer.fit(\n",
    "#         train_dataset, \n",
    "#         epochs=200, \n",
    "#         validation_data=val_dataset,\n",
    "#         callbacks=callback_list\n",
    "#     )\n",
    "\n",
    "\n",
    "# # Plot the loss history\n",
    "# plt.plot(ft_linear_history.history['output_accuracy'])\n",
    "# plt.plot(ft_linear_history.history['val_output_accuracy'])\n",
    "# plt.title('Model Acc')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Acc')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67578a8-e0fa-4e65-8aab-383f95cbfb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    ft_linear_transformer = tf.keras.models.load_model(f\"model/{DATASET}/classweight_ft_linear_transformer\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    ft_linear_transformer = tf.keras.models.load_model(f\"model/{DATASET}/scaled_ft_linear_transformer\")\n",
    "else:  \n",
    "    ft_linear_transformer = tf.keras.models.load_model(f\"model/{DATASET}/ft_linear_transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a01474-25dc-488f-b369-2fb54649f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "predictions = ft_linear_transformer.predict(test_dataset)\n",
    "y = y_test\n",
    "\n",
    "# Define the threshold for binary classification\n",
    "threshold = 0.5\n",
    "y_pred = (predictions[\"output\"] >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "roc_auc = roc_auc_score(y, y_pred)\n",
    "confusion_mat = confusion_matrix(y, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5b80c9-4c11-41b9-abf4-8830a7f10cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     ft_linear_transformer.save(f\"model/{DATASET}/classweight_ft_linear_transformer\", save_format=\"tf\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     ft_linear_transformer.save(f\"model/{DATASET}/scaled_ft_linear_transformer\", save_format=\"tf\")\n",
    "# else:  \n",
    "#     ft_linear_transformer.save(f\"model/{DATASET}/ft_linear_transformer\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67b2cb-d1bd-4305-92bc-3c659af33437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
