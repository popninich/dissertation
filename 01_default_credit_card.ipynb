{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd2809-021d-4cdf-a047-54c5c85d160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ee4d7-578b-49be-a97e-ae881410a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c804a33-7e11-4de0-bec0-8f8def3c3fde",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/improving-tabtransformer-part-1-linear-numerical-embeddings-dbc3be3b5bb5\n",
    "- https://github.com/aruberts/TabTransformerTF/blob/main/notebooks/fttransformer-demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e6da0f-c78d-48d8-b8fb-c857f9409afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"default of credit card clients\"\n",
    "\n",
    "# Available scenarios: original, standardization, imbalance_handle\n",
    "SCENARIO = \"standardization\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53444206-a001-4d61-8485-e61897be0845",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c065df-45ad-42ec-8ac8-c474dd35c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_excel(\n",
    "#     \"data/default of credit card clients.xls\", \n",
    "#     header=1, \n",
    "#     index_col=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c55ff8-4530-494d-b72a-22ee2f571b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde077c-7584-41f0-b879-343ac3d53a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    'LIMIT_BAL',\n",
    "    'SEX',\n",
    "    'EDUCATION',\n",
    "    'MARRIAGE',\n",
    "    'AGE',\n",
    "    'PAY_0',\n",
    "    'PAY_2',\n",
    "    'PAY_3',\n",
    "    'PAY_4',\n",
    "    'PAY_5',\n",
    "    'PAY_6',\n",
    "    'BILL_AMT1',\n",
    "    'BILL_AMT2',\n",
    "    'BILL_AMT3',\n",
    "    'BILL_AMT4',\n",
    "    'BILL_AMT5',\n",
    "    'BILL_AMT6',\n",
    "    'PAY_AMT1',\n",
    "    'PAY_AMT2',\n",
    "    'PAY_AMT3',\n",
    "    'PAY_AMT4',\n",
    "    'PAY_AMT5',\n",
    "    'PAY_AMT6',\n",
    "]\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    'LIMIT_BAL',\n",
    "    'AGE',\n",
    "    'PAY_0',\n",
    "    'PAY_2',\n",
    "    'PAY_3',\n",
    "    'PAY_4',\n",
    "    'PAY_5',\n",
    "    'PAY_6',\n",
    "    'BILL_AMT1',\n",
    "    'BILL_AMT2',\n",
    "    'BILL_AMT3',\n",
    "    'BILL_AMT4',\n",
    "    'BILL_AMT5',\n",
    "    'BILL_AMT6',\n",
    "    'PAY_AMT1',\n",
    "    'PAY_AMT2',\n",
    "    'PAY_AMT3',\n",
    "    'PAY_AMT4',\n",
    "    'PAY_AMT5',\n",
    "    'PAY_AMT6',\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'SEX',\n",
    "    'EDUCATION',\n",
    "    'MARRIAGE',\n",
    "]\n",
    "LABEL = \"default payment next month\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca679c8c-50f6-47a4-96f9-f9f42bf90233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[FEATURES]\n",
    "# y = df[LABEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e4fa5-87ec-46de-8e6e-76345ea07ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the shape of the training and testing sets\n",
    "# print(\"Dataset shape:\", X.shape, y.shape)\n",
    "\n",
    "# # There are imbalanced data in the y_train\n",
    "# print(\"Label Distribution in y_train:\")\n",
    "# print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c73a0-4e41-4645-8979-0cd095babf26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5351f9bb-a468-4a2b-b197-a01f501f4aec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac5190-e320-47ec-a21e-01ca099919b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468147ba-0136-446c-8329-1efb124b05fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply label encoder to object columns\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for col in CATEGORICAL_FEATURES:\n",
    "#     X.loc[:, col] = label_encoder.fit_transform(X.loc[:, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f4615-6c7f-4e30-8cae-00a8017b55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply normalizer\n",
    "# if SCENARIO in (\"standardization\", \"imbalance_handle\"):\n",
    "#     scaler = StandardScaler()\n",
    "    \n",
    "#     # Normalize the numeric columns\n",
    "#     X.loc[:, NUMERIC_FEATURES] = scaler.fit_transform(X.loc[:, NUMERIC_FEATURES])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6bd3e8-9670-42f0-8cd0-0e4d680b7301",
   "metadata": {},
   "source": [
    "### Split train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4247a2-e3cd-48bf-bb96-08644c937fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the data into training, validation, and testing sets: 70, 10, 20%\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    "# )\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_train_val, y_train_val, test_size=0.125, stratify=y_train_val, random_state=SEED\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df48a71f-c377-4ca7-ab40-05c8f5d93d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if SCENARIO in (\"standardization\", \"imbalance_handle\"):\n",
    "#     X_train.to_csv(f\"data/X_y/{DATASET}/scaled_X_train.csv\")\n",
    "#     y_train.to_csv(f\"data/X_y/{DATASET}/scaled_y_train.csv\")\n",
    "    \n",
    "#     X_val.to_csv(f\"data/X_y/{DATASET}/scaled_X_val.csv\")\n",
    "#     y_val.to_csv(f\"data/X_y/{DATASET}/scaled_y_val.csv\")\n",
    "    \n",
    "#     X_test.to_csv(f\"data/X_y/{DATASET}/scaled_X_test.csv\")\n",
    "#     y_test.to_csv(f\"data/X_y/{DATASET}/scaled_y_test.csv\")\n",
    "    \n",
    "# else:\n",
    "#     X_train.to_csv(f\"data/X_y/{DATASET}/X_train.csv\")\n",
    "#     y_train.to_csv(f\"data/X_y/{DATASET}/y_train.csv\")\n",
    "    \n",
    "#     X_val.to_csv(f\"data/X_y/{DATASET}/X_val.csv\")\n",
    "#     y_val.to_csv(f\"data/X_y/{DATASET}/y_val.csv\")\n",
    "    \n",
    "#     X_test.to_csv(f\"data/X_y/{DATASET}/X_test.csv\")\n",
    "#     y_test.to_csv(f\"data/X_y/{DATASET}/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fbc65b-5164-42b6-b1f0-cd440af3f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCENARIO in (\"standardization\", \"imbalance_handle\"):\n",
    "    X_train = pd.read_csv(f\"data/X_y/{DATASET}/scaled_X_train.csv\", index_col=0)\n",
    "    X_val = pd.read_csv(f\"data/X_y/{DATASET}/scaled_X_val.csv\", index_col=0)\n",
    "    X_test = pd.read_csv(f\"data/X_y/{DATASET}/scaled_X_test.csv\", index_col=0)\n",
    "    \n",
    "    y_train = pd.read_csv(f\"data/X_y/{DATASET}/scaled_y_train.csv\", index_col=0)\n",
    "    y_val = pd.read_csv(f\"data/X_y/{DATASET}/scaled_y_val.csv\", index_col=0)\n",
    "    y_test = pd.read_csv(f\"data/X_y/{DATASET}/scaled_y_test.csv\", index_col=0)\n",
    "\n",
    "else:\n",
    "    X_train = pd.read_csv(f\"data/X_y/{DATASET}/X_train.csv\", index_col=0)\n",
    "    X_val = pd.read_csv(f\"data/X_y/{DATASET}/X_val.csv\", index_col=0)\n",
    "    X_test = pd.read_csv(f\"data/X_y/{DATASET}/X_test.csv\", index_col=0)\n",
    "    \n",
    "    y_train = pd.read_csv(f\"data/X_y/{DATASET}/y_train.csv\", index_col=0)\n",
    "    y_val = pd.read_csv(f\"data/X_y/{DATASET}/y_val.csv\", index_col=0)\n",
    "    y_test = pd.read_csv(f\"data/X_y/{DATASET}/y_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3933b5df-b818-492a-a856-cb87955a9ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "print()\n",
    "\n",
    "# There are imbalanced data in the y_train\n",
    "print(\"Label Distribution in y_train:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260beaa-4043-482d-9b62-0aa38df132e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887cc888-59c3-4953-9945-fc0c7cbe2d61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae2a07f-36b1-4901-9b76-fe42ea91effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dee0c1-b8ff-4e78-9c80-4e7dc8044af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the imbalance handling\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Assuming train_labels contains the integer class labels of your training data.\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train[LABEL].values)\n",
    "\n",
    "# Convert class_weights to a dictionary\n",
    "class_weight_dict = {class_index: weight for class_index, weight in enumerate(class_weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403a9f9-c7c7-4772-a3dc-f21b450ef872",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"class_weight_dict: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9870abda-809b-4469-bbfe-0162478436c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd3b683-c71b-47ec-a66b-915a0264d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     logistic_regression = LogisticRegression(class_weight=class_weight_dict)\n",
    "#     decision_tree = DecisionTreeClassifier(class_weight=class_weight_dict)\n",
    "#     random_forest = RandomForestClassifier(class_weight=class_weight_dict)\n",
    "#     svm_classifier = SVC(class_weight=class_weight_dict)\n",
    "# else:  \n",
    "#     logistic_regression = LogisticRegression()\n",
    "#     decision_tree = DecisionTreeClassifier()\n",
    "#     random_forest = RandomForestClassifier()\n",
    "#     svm_classifier = SVC()\n",
    "\n",
    "# # Train the classifiers\n",
    "# logistic_regression.fit(X_train, y_train)\n",
    "# decision_tree.fit(X_train, y_train)\n",
    "# random_forest.fit(X_train, y_train)\n",
    "# svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50521545-1b2d-4a48-b0af-7a1a9a31d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    logistic_regression = joblib.load(f\"model/{DATASET}/classweight_logistic_regression.joblib\")\n",
    "    decision_tree = joblib.load(f\"model/{DATASET}/classweight_decision_tree.joblib\")\n",
    "    random_forest = joblib.load(f\"model/{DATASET}/classweight_random_forest.joblib\")\n",
    "    svm_classifier = joblib.load(f\"model/{DATASET}/classweight_svm_classifier.joblib\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    logistic_regression = joblib.load(f\"model/{DATASET}/scaled_logistic_regression.joblib\")\n",
    "    decision_tree = joblib.load(f\"model/{DATASET}/scaled_decision_tree.joblib\")\n",
    "    random_forest = joblib.load(f\"model/{DATASET}/scaled_random_forest.joblib\")\n",
    "    svm_classifier = joblib.load(f\"model/{DATASET}/scaled_svm_classifier.joblib\")\n",
    "else:  \n",
    "    logistic_regression = joblib.load(f\"model/{DATASET}/logistic_regression.joblib\")\n",
    "    decision_tree = joblib.load(f\"model/{DATASET}/decision_tree.joblib\")\n",
    "    random_forest = joblib.load(f\"model/{DATASET}/random_forest.joblib\")\n",
    "    svm_classifier = joblib.load(f\"model/{DATASET}/svm_classifier.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec26fc-9533-496f-b74e-38d1c5b998fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "models = {\n",
    "    \"Logistic Regression\": logistic_regression,\n",
    "    \"Decision Tree\": decision_tree,\n",
    "    \"Random Forest\": random_forest,\n",
    "    \"SVM\": svm_classifier\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"Model:\", model_name)\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1-score: {f1:.3f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "    print(confusion_matrix(y_test, y_pred)) # tn, fp, fn, tp\n",
    "    # print(classification_report(y_test, y_pred))\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4249e1-d919-49a7-b283-224a2b499b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     joblib.dump(logistic_regression, f\"model/{DATASET}/classweight_logistic_regression.joblib\")\n",
    "#     joblib.dump(decision_tree, f\"model/{DATASET}/classweight_decision_tree.joblib\")\n",
    "#     joblib.dump(random_forest, f\"model/{DATASET}/classweight_random_forest.joblib\")\n",
    "#     joblib.dump(svm_classifier, f\"model/{DATASET}/classweight_svm_classifier.joblib\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     joblib.dump(logistic_regression, f\"model/{DATASET}/scaled_logistic_regression.joblib\")\n",
    "#     joblib.dump(decision_tree, f\"model/{DATASET}/scaled_decision_tree.joblib\")\n",
    "#     joblib.dump(random_forest, f\"model/{DATASET}/scaled_random_forest.joblib\")\n",
    "#     joblib.dump(svm_classifier, f\"model/{DATASET}/scaled_svm_classifier.joblib\")\n",
    "# else:  \n",
    "#     joblib.dump(logistic_regression, f\"model/{DATASET}/logistic_regression.joblib\")\n",
    "#     joblib.dump(decision_tree, f\"model/{DATASET}/decision_tree.joblib\")\n",
    "#     joblib.dump(random_forest, f\"model/{DATASET}/random_forest.joblib\")\n",
    "#     joblib.dump(svm_classifier, f\"model/{DATASET}/svm_classifier.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0968f-5fe7-4e41-8119-7e3824f4f752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d65b5a2c-3995-403c-87af-eaf569f2c334",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96449cf8-469c-4c03-9c73-a2db07b43b79",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed1234-3120-4a0b-b25b-e4a9bc005dc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train Neural networks\n",
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.Dense(32, activation='relu', input_shape=(len(FEATURES),)))\n",
    "# model.add(keras.layers.Dense(8, activation='relu'))\n",
    "# model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# early_stopper = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=50, restore_best_weights=True)\n",
    "\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     history = model.fit(\n",
    "#         X_train,\n",
    "#         y_train, \n",
    "#         epochs=200, \n",
    "#         validation_data=(X_val, y_val), \n",
    "#         callbacks=[early_stopper], \n",
    "#         class_weight=class_weight_dict\n",
    "#     )\n",
    "# else:\n",
    "#     history = model.fit(\n",
    "#         X_train,\n",
    "#         y_train, \n",
    "#         epochs=200, \n",
    "#         validation_data=(X_val, y_val), \n",
    "#         callbacks=[early_stopper]\n",
    "#     )\n",
    "\n",
    "# # Plot the loss history\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e23b4-803a-4cbc-98b9-0230465b8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    model = tf.keras.models.load_model(f\"model/{DATASET}/classweight_neural_network.h5\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    model = tf.keras.models.load_model(f\"model/{DATASET}/scaled_neural_network.h5\")\n",
    "else:  \n",
    "    model = tf.keras.models.load_model(f\"model/{DATASET}/neural_network.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494be9b5-b817-47a5-802d-938b0f34a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Define the threshold for binary classification\n",
    "threshold = 0.5\n",
    "y_pred = (predictions >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "print(confusion_mat)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a755e6-1269-4669-a0da-a1378e7028dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     model.save(f\"model/{DATASET}/classweight_neural_network.h5\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     model.save(f\"model/{DATASET}/scaled_neural_network.h5\")\n",
    "# else:  \n",
    "#     model.save(f\"model/{DATASET}/neural_network.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749bc3c9-bd68-4071-8cd1-bb54c3674d4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774415de-9713-43c3-a6ca-872209013270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train CNN\n",
    "# cnn_model = Sequential()\n",
    "# cnn_model.add(Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "# cnn_model.add(MaxPooling1D(2))\n",
    "# cnn_model.add(Flatten())\n",
    "# cnn_model.add(Dense(128, activation='relu'))\n",
    "# cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "# cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Convert the data to the required input shape for CNN (num_samples, num_features, num_channels)\n",
    "# X_train_cnn = np.expand_dims(X_train, axis=2)\n",
    "# X_val_cnn = np.expand_dims(X_val, axis=2)\n",
    "\n",
    "# early_stopper = EarlyStopping(patience=50, restore_best_weights=True)\n",
    "\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     history = cnn_model.fit(\n",
    "#         X_train_cnn, y_train, \n",
    "#         epochs=500, \n",
    "#         validation_data=(X_val_cnn, y_val), \n",
    "#         callbacks=[early_stopper],\n",
    "#         class_weight=class_weight_dict\n",
    "#     )\n",
    "# else:\n",
    "#     history = cnn_model.fit(\n",
    "#         X_train_cnn, y_train, \n",
    "#         epochs=500, \n",
    "#         validation_data=(X_val_cnn, y_val), \n",
    "#         callbacks=[early_stopper]\n",
    "#     )\n",
    "\n",
    "# # Plot the loss history\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539ad02-916d-4494-a4d0-ecb0fe3f1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    cnn_model = tf.keras.models.load_model(f\"model/{DATASET}/classweight_cnn.h5\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    cnn_model = tf.keras.models.load_model(f\"model/{DATASET}/scaled_cnn.h5\")\n",
    "else:  \n",
    "    cnn_model = tf.keras.models.load_model(f\"model/{DATASET}/cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e87664-fa6b-410e-9575-5dc4b9f5c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "X_test_cnn = np.expand_dims(X_test, axis=2)\n",
    "predictions = cnn_model.predict(X_test_cnn)\n",
    "\n",
    "# Define the threshold for binary classification\n",
    "threshold = 0.5\n",
    "y_pred = (predictions >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "print(confusion_mat)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1396c20-3536-49e6-8d5e-9417e6302587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     cnn_model.save(f\"model/{DATASET}/classweight_cnn.h5\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     cnn_model.save(f\"model/{DATASET}/scaled_cnn.h5\")\n",
    "# else:  \n",
    "#     cnn_model.save(f\"model/{DATASET}/cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace90c05-1b54-4a01-97cb-dd4f9138ba05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "199880c1-b88a-4ed5-afaa-3e4fb836b3c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7913212-5940-494d-8c33-d8db8478085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input data for the LSTM model\n",
    "input_dim = len(FEATURES)\n",
    "input_shape = (input_dim, 1)  # Add an extra dimension for LSTM input\n",
    "\n",
    "# Reshape the input data for LSTM\n",
    "X_train_lstm = X_train.values.reshape((-1, input_dim, 1))\n",
    "X_val_lstm = X_val.values.reshape((-1, input_dim, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc118b4b-01f6-4b5d-9592-20965c20ef01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train the LSTMs\n",
    "# inputs = Input(shape=input_shape)\n",
    "# lstm_layer = LSTM(32, activation='relu')(inputs)\n",
    "# dropout_layer = Dropout(0.1)(lstm_layer)\n",
    "# outputs = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "# lstm_model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# # Compile the model\n",
    "# lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# early_stopper = EarlyStopping(patience=15, restore_best_weights=True)\n",
    "\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     history = lstm_model.fit(\n",
    "#         X_train_lstm, y_train, \n",
    "#         epochs=100, \n",
    "#         validation_data=(X_val_lstm, y_val), \n",
    "#         callbacks=[early_stopper],\n",
    "#         class_weight=class_weight_dict\n",
    "#     )\n",
    "# else:\n",
    "#     history = lstm_model.fit(\n",
    "#         X_train_lstm, y_train, \n",
    "#         epochs=100, \n",
    "#         validation_data=(X_val_lstm, y_val), \n",
    "#         callbacks=[early_stopper]\n",
    "#     )\n",
    "\n",
    "# # Plot the loss history\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e418570d-8857-4410-a409-bfdc8539b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    lstm_model = tf.keras.models.load_model(f\"model/{DATASET}/classweight_lstm.h5\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    lstm_model = tf.keras.models.load_model(f\"model/{DATASET}/scaled_lstm.h5\")\n",
    "else:  \n",
    "    lstm_model = tf.keras.models.load_model(f\"model/{DATASET}/lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb0e0b-0ba4-475f-ab6c-dc01d56daa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "X_test_lstm = X_test.values.reshape((-1, input_dim, 1))\n",
    "predictions = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "# Define the threshold for binary classification\n",
    "threshold = 0.5\n",
    "y_pred = (predictions >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "print(confusion_mat)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3152cef5-73a5-466f-b296-7029f63e1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     lstm_model.save(f\"model/{DATASET}/classweight_lstm.h5\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     lstm_model.save(f\"model/{DATASET}/scaled_lstm.h5\")\n",
    "# else:  \n",
    "#     lstm_model.save(f\"model/{DATASET}/lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4620be7-18a0-4d02-9611-e0aade401d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabtransformertf==0.0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e7400-ef9a-44d8-8e27-067f4a66e644",
   "metadata": {},
   "source": [
    "### TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878944f-60a5-45b6-b31a-9c33bd1b91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "from tabtransformertf.models.tabtransformer import TabTransformer\n",
    "from tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73daca3-ea03-4b50-b2c9-fba19c5e7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(\n",
    "    dataframe: pd.DataFrame,\n",
    "    target: str = None,\n",
    "    shuffle: bool = True,\n",
    "    batch_size: int = 512,\n",
    "):\n",
    "    df = dataframe.copy()\n",
    "    if target:\n",
    "        labels = df.pop(target)\n",
    "        dataset = {}\n",
    "        for key, value in df.items():\n",
    "            dataset[key] = tf.expand_dims(value, axis=1) # Expand dimension similar to value[:, tf.newaxis]\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((dict(dataset), labels))\n",
    "    else:\n",
    "        dataset = {}\n",
    "        for key, value in df.items():\n",
    "            dataset[key] = tf.expand_dims(value, axis=1)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(dict(dataset))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d011ce-eb21-466b-a040-ee8fc6e93506",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_X_train = X_train.copy()\n",
    "tab_X_val = X_val.copy()\n",
    "tab_X_test = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e4aac6-ce48-4cf2-a25c-a3c6b0cdd158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEX: Gender (1=male, 2=female)\n",
    "# EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
    "# MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
    "\n",
    "tab_X_train[\"SEX\"] = tab_X_train[\"SEX\"].map({\n",
    "    1: \"male\", 2: \"female\"\n",
    "}).fillna(\"null\")\n",
    "tab_X_val[\"SEX\"] = tab_X_val[\"SEX\"].map({\n",
    "    1: \"male\", 2: \"female\"\n",
    "}).fillna(\"null\")\n",
    "tab_X_test[\"SEX\"] = tab_X_test[\"SEX\"].map({\n",
    "    1: \"male\", 2: \"female\"\n",
    "}).fillna(\"null\")\n",
    "\n",
    "tab_X_train[\"EDUCATION\"] = tab_X_train[\"EDUCATION\"].map({\n",
    "    1: \"graduate school\", 2: \"university\", 3: \"high school\", 4: \"others\", 5: \"unknown\", 6: \"unknown\"\n",
    "}).fillna(\"null\")\n",
    "tab_X_val[\"EDUCATION\"] = tab_X_val[\"EDUCATION\"].map({\n",
    "    1: \"graduate school\", 2: \"university\", 3: \"high school\", 4: \"others\", 5: \"unknown\", 6: \"unknown\"\n",
    "}).fillna(\"null\")\n",
    "tab_X_test[\"EDUCATION\"] = tab_X_test[\"EDUCATION\"].map({\n",
    "    1: \"graduate school\", 2: \"university\", 3: \"high school\", 4: \"others\", 5: \"unknown\", 6: \"unknown\"\n",
    "}).fillna(\"null\")\n",
    "\n",
    "tab_X_train[\"MARRIAGE\"] = tab_X_train[\"MARRIAGE\"].map({\n",
    "    1: \"married\", 2: \"single\", 3: \"others\"\n",
    "}).fillna(\"null\")\n",
    "tab_X_val[\"MARRIAGE\"] = tab_X_val[\"MARRIAGE\"].map({\n",
    "    1: \"married\", 2: \"single\", 3: \"others\"\n",
    "}).fillna(\"null\")\n",
    "tab_X_test[\"MARRIAGE\"] = tab_X_test[\"MARRIAGE\"].map({\n",
    "    1: \"married\", 2: \"single\", 3: \"others\"\n",
    "}).fillna(\"null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8168b5-5413-4e30-9d99-84293fb69f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([tab_X_train, y_train], axis=1)\n",
    "val_df = pd.concat([tab_X_val, y_val], axis=1)\n",
    "test_df = pd.concat([tab_X_test, y_test], axis=1)\n",
    "\n",
    "# Set data types\n",
    "train_df[CATEGORICAL_FEATURES] = train_df[CATEGORICAL_FEATURES].astype(str)\n",
    "val_df[CATEGORICAL_FEATURES] = val_df[CATEGORICAL_FEATURES].astype(str)\n",
    "test_df[CATEGORICAL_FEATURES] = test_df[CATEGORICAL_FEATURES].astype(str)\n",
    "\n",
    "train_df[NUMERIC_FEATURES] = train_df[NUMERIC_FEATURES].astype(float)\n",
    "val_df[NUMERIC_FEATURES] = val_df[NUMERIC_FEATURES].astype(float)\n",
    "test_df[NUMERIC_FEATURES] = test_df[NUMERIC_FEATURES].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56e6e8-3713-4a98-be87-bd50872c4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To TF Dataset\n",
    "# Default batch_size = 32\n",
    "train_dataset = df_to_dataset(train_df, LABEL, batch_size=32, shuffle=False)\n",
    "val_dataset = df_to_dataset(val_df, LABEL, batch_size=32, shuffle=False) # No shuffle\n",
    "test_dataset = df_to_dataset(test_df, LABEL, batch_size=32, shuffle=False) # No shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b91bde-376c-41a2-9436-9ce0b8b3aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053afe95-a463-4bfd-935f-645c07d01b72",
   "metadata": {},
   "source": [
    "#### fttransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa150422-ea17-4228-ba58-0b8b3cc1db81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train the tab transformer\n",
    "# ft_linear_encoder = FTTransformerEncoder(\n",
    "#     numerical_features=NUMERIC_FEATURES,\n",
    "#     categorical_features=CATEGORICAL_FEATURES,\n",
    "#     numerical_data=train_df[NUMERIC_FEATURES].values,\n",
    "#     categorical_data=train_df[CATEGORICAL_FEATURES].values,\n",
    "#     y=None,\n",
    "#     numerical_embedding_type='linear',\n",
    "#     embedding_dim=16,\n",
    "#     depth=4,\n",
    "#     heads=8,\n",
    "#     attn_dropout=0.2,\n",
    "#     ff_dropout=0.2,\n",
    "#     explainable=True\n",
    "# )\n",
    "\n",
    "# # Pass the encoder to the model\n",
    "# ft_linear_transformer = FTTransformer(\n",
    "#     encoder=ft_linear_encoder,\n",
    "#     out_dim=1,\n",
    "#     out_activation='sigmoid',\n",
    "# )\n",
    "\n",
    "# ft_linear_transformer.compile(\n",
    "#     optimizer=\"adam\",\n",
    "#     loss=\"binary_crossentropy\",\n",
    "#     metrics=[\"accuracy\"]\n",
    "# )\n",
    "\n",
    "# early = EarlyStopping(monitor=\"val_output_loss\", mode=\"min\", patience=20, restore_best_weights=True)\n",
    "# callback_list = [early]\n",
    "\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     ft_linear_history = ft_linear_transformer.fit(\n",
    "#         train_dataset, \n",
    "#         epochs=200, \n",
    "#         validation_data=val_dataset,\n",
    "#         callbacks=callback_list,\n",
    "#         class_weight=class_weight_dict\n",
    "#     )\n",
    "# else:\n",
    "#     ft_linear_history = ft_linear_transformer.fit(\n",
    "#         train_dataset, \n",
    "#         epochs=200, \n",
    "#         validation_data=val_dataset,\n",
    "#         callbacks=callback_list\n",
    "#     )\n",
    "\n",
    "\n",
    "# # Plot the loss history\n",
    "# plt.plot(ft_linear_history.history['output_accuracy'])\n",
    "# plt.plot(ft_linear_history.history['val_output_accuracy'])\n",
    "# plt.title('Model Acc')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Acc')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a00ad-a93e-41b6-b93b-6066cde99833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifiers\n",
    "if SCENARIO == \"imbalance_handle\":\n",
    "    ft_linear_transformer = tf.keras.models.load_model(f\"model/{DATASET}/classweight_ft_linear_transformer\")\n",
    "elif SCENARIO == \"standardization\":\n",
    "    ft_linear_transformer = tf.keras.models.load_model(f\"model/{DATASET}/scaled_ft_linear_transformer\")\n",
    "else:  \n",
    "    ft_linear_transformer = tf.keras.models.load_model(f\"model/{DATASET}/ft_linear_transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a01474-25dc-488f-b369-2fb54649f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "predictions = ft_linear_transformer.predict(test_dataset)\n",
    "y = y_test\n",
    "\n",
    "# Define the threshold for binary classification\n",
    "threshold = 0.5\n",
    "y_pred = (predictions[\"output\"] >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "roc_auc = roc_auc_score(y, y_pred)\n",
    "confusion_mat = confusion_matrix(y, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "print(confusion_mat)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5b80c9-4c11-41b9-abf4-8830a7f10cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the classifiers\n",
    "# if SCENARIO == \"imbalance_handle\":\n",
    "#     ft_linear_transformer.save(f\"model/{DATASET}/classweight_ft_linear_transformer\", save_format=\"tf\")\n",
    "# elif SCENARIO == \"standardization\":\n",
    "#     ft_linear_transformer.save(f\"model/{DATASET}/scaled_ft_linear_transformer\", save_format=\"tf\")\n",
    "# else:  \n",
    "#     ft_linear_transformer.save(f\"model/{DATASET}/ft_linear_transformer\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48993e3-be14-4478-8f59-bedc8eeec2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
